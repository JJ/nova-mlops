---
title: "Programming infrastructure"
author: "JJ Merelo"
date: "3/7/2022"
output: html_document
bibliography: ../mlops.bib
---
# Programming infrastructure for MLOps

## TL;DR

MLOps tries to create continuously deployable workflows, and that
requires tests, checks and scripts chained that will be run in a CI/CD
environment. We will learn how to work with these, focusing in the
most popular tool nowadays, GitHub Actions.

## Learning outcomes of this unit

The student will learn different concepts associated with CI/CD
workflows, and will be able to create their own using GitHub actions.

## Acceptance criteria

The student must be familiar with the concepts involving CI/CD and how to use
readily available ones for data science/machine learning and will have used them
in the data science project that's been developed for this course.

## CI/CD for the masses

This step is related to all the stages that have *continuous* in their name in
the MLOps ladder: continuour training means that, as new data comes in, it needs
to be either fed into the model for training or simply passed through the ETL
pipeline to check for health and eventually represent it in some sensible way.

In most cases, cloud providers and software as a service (SaaS) companies
will try to offer you a full-span solution for MLOps, where you will have to pay
every step of the way. It will also be a monolithic solution where the onramp to
start and using it from scratch, as well as stopping its use or moving it to
other tool, can be extremely complicated or even impossible.

In general, however, creating MLOps workflows can be done easily and
for free if you are working with free software and your repositories have been
released with a free license.

MLOps, in general, is strongly related to the movement for public
money, public software, as well as open science. So working in public
repositories is the right thing to do if you are in Academia; it is
probably a very good idea if you are in industry and you are not
working with proprietary data sets.



## Activity

Come this point, different parts of the machine learning pipeline, like
downloading and processing data, must be already available. A series of
workflows using GitHub actions must be created for

* Automatically testing existing code.
* Testing data once downloaded.
* Downloading automatically data with a certain periodicity if needed.

Different workflows must be triggered by code and by data changes, and changes
in the repository should be made automatically by these workflows without human
intervention.

## References
