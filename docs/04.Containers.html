<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="English" xml:lang="English">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>MlOps for Nova: Reproducible science using containers</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" href="mlops.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h2>MLOps course for Nova-Lisbon. <a href='index.html'>Index</a> | <a href='https://github.com/JJ/nova-mlops'>Github Repository</a></h2>
<h1 id="reproducible-science-using-containers">Reproducible science using containers</h1>
<h2 id="tldr">TL;DR</h2>
<p>In an open science context, containers are not only good for registering the infrastructure in MLOps, but also because they can be easily published and enable anyone to re-use them. We will see best practices in the creation of <em>active</em> and data containers that can be easily used in ML workflows.</p>
<h2 id="learning-outcomes-of-this-unit">Learning outcomes of this unit</h2>
<p>Students will learn how to use containers for deployment of models, as well as to store and possibly publish data.</p>
<h2 id="acceptance-criteria">Acceptance criteria</h2>
<p>Several data and active containers have been created and integrated into the publishing schedule as well as different workflows.</p>
<h2 id="containers">Containers</h2>
<p>Containers were one of the factors that brought the DevOps (quasi) revolution to the software engineering world. The fact that they allowed to isolate the execution of some programs or services, that they could be packaged and stored, but also that they included a language that allowed very easily to describe how to create them, was a real stunner.</p>
<p>Essentially, those are the three factors that make them useful also for data science: you can isolate an application from the rest of the system, in such a way that it is very easy to deploy it anywhere; that isolation comes at the price of precisely describing the infrastructure that will be needed for running that application. But the prize is that once that is done, you can deploy your application, be it a MLOps stage, or even data, wherever you want.</p>
<p>Let’s check out the concepts that go together with containers, or Docker containers as they are called, since that was the company that created and released the framework for using them in the first place.</p>
<p>First and foremost, what containers do is to <em>isolate</em> applications, not virtualize services. The applications will still use the services of the host operating system, mainly related to that isolation. Which services does not matter too much, what matters is that the operating system must have support for them, and so far only Linux, Windows and some other IBM operating systems do. Also, they do not virtualize the processor instruction set, they run executables on the host processor. That implies that you will need a specific combination of operating system and processor architecture in the host to run a container (and the other way round).</p>
<p>However, containers are a standard (from OCI, open container initiative) and attempt to create a system that any developer can use and that can be then deployed anywhere. How does that work? The software you download on the operating system has two parts:</p>
<ul>
<li>The command line, or graphical front end, which obviously works on your machine as a native program. This is used mainly for management.</li>
<li>The service or daemon, which runs with privileges, and which connects to the client through a socket. This means that the service can, in principle, run anywhere, as long as you connect to it. This service also acts as <em>registry</em>, holding a list of your containers. This can be a virtual machine, and it is how it works on Macs (whose operating system has no support for isolation) and in Windows (where you will very probably create containers for Linux anyway).</li>
</ul>
<p>Let’s see different states a container can be in.</p>
<ul>
<li>You create a container usually through a Dockerfile, at least in this context (it can be created interactively, but except for prototyping, it’s not really advised to do so). This Dockerfile has specific commands to copy files, run scripts, and define what will be run when the image is invoked.</li>
<li>From the Dockerfile, you build an <em>image</em>; this process is similar to compiling to a runnable file, and it creates a file that can be downloaded and run using any Docker client you have.</li>
<li>Finally, a <em>container</em> is the equivalent of a running executable. It receives commands, or runs and exits.</li>
</ul>
<p>The general best practice is to use Dockerfiles in your project to create images, that are stored in registries (Docker Hub, GitHub Container Registry, Quay.io)</p>
<p>Good thing about containers is that you have a wide array of them to be used directly. All containers are built from a base, and these bases are published in registries, so you can just look for one and start executing it.</p>
<blockquote>
<p>At this point, you should have an installation of Docker in your machine. Please follow instructions for your specific platform. In general we will working with Linux/Intel containers, so if your platform offer several options, use the one that is able to work with this kind of containers.</p>
</blockquote>
<p>For instance, we can start running a Jupyter Hub just by doing this:</p>
<pre><code>docker run --rm opendatacube/jupyterhub  jupyterhub --ip=&quot;*&quot;</code></pre>
<blockquote>
<p>There’s probably a GUI option to do more or less the same.</p>
</blockquote>
<p>The first part, <code>docker run</code>, is what effectively tells the service we are going to run a container; this will also check if the container is already in our machine, and download it if it is not. You will see that it will show a message about pulling, and several hexadecimal hashes that represents the different layers that compose the image. It does not really matter a lot right now, but best practices advise to minimize the amount of layers you use.</p>
<p>The rest of the command line is as follows</p>
<ul>
<li><code>--rm</code> will delete the container when you’ve finished running it. This is a very convenient way to not have lots of stopped containers in your local hard disk, and it’s the usual way to do this. It will still keep the <em>image</em> so that you will not have to download it again (in fact, it will store the <em>layers</em>, so it will speed up downloading for images that <em>depend</em> on this one).</li>
<li><code>opendatacube/jupyterhub</code> this is the name of the image, usually composed by a namespace (or publishers) and a specific name for the image; it can be followed by a tag in the shape <code>:tag</code>.</li>
<li><code>jupyterhub --ip='*'</code> is a command you issue, or an argument you give the “executable” contained there.</li>
</ul>
<p>Jupyter <a href="https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html#jupyter-datascience-notebook">releases a number of notebooks</a> you can readily use, that already have preinstalled a series of modules used in data science. For instance, this one:</p>
<pre><code>docker run --rm -p8888:8888 jupyter/scipy-notebook</code></pre>
<p>will directly run a notebook with <code>scipy</code> preinstalled; the lab will be accessible in the 8888 port, which you have mapped to an internal port via the command <code>-p</code>.</p>
<p>In most cases, containers are not created to be used directly; they will be the <em>base</em> for other applications that will add more <em>layers</em> to them, including the files and other dependencies your application will need. We will see how to build these images ourselves right next.</p>
<h2 id="creating-our-own-containers">Creating our own containers</h2>
<p>We can create our own containers for multitude of purposes. One of the simplest one we can create is to hold data. There are three parts on what we need to do to design and build a container</p>
<ul>
<li>Decide on base container. Base containers will have the basic tools we need: the language, for instance. In some cases, we will use tools that include the base “operating system”; which is not a complete operating system, because it will use the kernel, or core, of the host operating system. Since you want your container to have the minimum amount of layers, you need the operating system to be as lightweight as possible; this is why some Linux distributions publish their own lightweight versions, like Debian Slim. Some, like Alpine, are specifically designed to be lightweight. In case you choose a higher-level image, like one that is officially created for a language, look for those tagged <code>alpine</code> or <code>slim</code>. Deciding on this base container will imply choices when installing other tools on it and how to do it. At any rate, the choice of base image has <em>nothing</em> to do with your preferences or what you use on your machine. Main criteria is functionality and weight.</li>
<li>Decide which files will go into the image. This is usually easy, although it sometimes has two phases: copying into the yet-to-be-built image some files, and then performing some processing (compilation or other) to generate the files that <em>actually</em> go into the image; you can then proceed to delete or eliminate those files. This will imply also creating a filesystem structure that we will need for several purposes, mainly the application’s own purposes, as well as interacting through a shared directory with the host or other containers.</li>
<li>Find out the command that the container is going to run when invoked with <code>docker run</code>. You have a single chance (there’s only a command than can be invoked) so choose wisely.</li>
</ul>
<p>For instance, this (relatively simple) container has been created to hold the data from the ministry of Defense in the war of Ukraine.</p>
<pre><code>FROM denoland/deno:latest

LABEL version=&quot;1.0.0&quot; maintainer=&quot;JJMerelo@GMail.com&quot;

WORKDIR /app
COPY tools/serve-data.ts .
RUN mkdir resources
COPY resources/*.csv resources/

EXPOSE 31415
VOLUME resources

CMD [&quot;run&quot;, &quot;--allow-net&quot;, &quot;--allow-read&quot;,  &quot;serve-data.ts&quot;]</code></pre>
<p>Besides the data, it contains a script in Deno that serves the data via a web server; this explains why we use a container for Deno in the <code>FROM</code> statement.</p>
<p><code>LABEL</code> is mainly for metadata, not really functional; the next 4 sentences decide where the application is going to be run (<code>WORKDIR</code>), copy the Deno script that serves the data, and then creates a subdirectory for resources and copies them from the host (first argument) to the container (<code>resources/</code>, the director will be already created).</p>
<p>Next two sentences are also metadata: it tells you what is the name of the shared directory you should use, and the port the server is going to use (prize if you recognize the number).</p>
<p>Finally, the last part is what you are going to run: using Deno, it runs the server that will serve the data. This image is published in the <a href="https://github.com/JJ/raku-ukr-mod-data/pkgs/container/ukr-mod-data">Github registry</a>, and you can use it directly like this:</p>
<pre><code>docker pull -p31415:31415 ghcr.io/jj/ukr-mod-data:latest</code></pre>
<p>In a Russian doll manner, you can build your containers using as base (that is, using the <code>FROM</code> command) other, existing containers, or, since they are in most cases open source, inspiring yourself by them to create your own. For instance, this uses the code in the course itself to contain data and just print it when invoked:</p>
<pre><code>FROM python:latest

LABEL version=0.0.1 maintainer=&quot;jjmerelo@gmail.com&quot;
RUN useradd -ms /bin/bash novamlops
USER novamlops
WORKDIR /home/novamlops
ENV PATH=~&quot;/home/novamlops/.poetry/bin:/home/novamlops/.local/bin:${PATH}&quot;

COPY --chown=novamlops pyproject.toml poetry.lock .
RUN mkdir colares_project
COPY --chown=novamlops colares_project/ colares_project/
RUN pip install poetry \
    &amp;&amp; poetry install \
    &amp;&amp; poetry run testcsv \
    &amp;&amp; rm -rf colares_project/ pyproject.toml poetry.lock

ENTRYPOINT cat Export_test.csv</code></pre>
<p>It’s similar to the one above, except for a couple of details</p>
<ul>
<li>For security reasons, it is better if whatever runs in the container runs as a non-privileged user. This is why we create an user, <code>novamlops</code>, and run everything (including <code>COPY</code> as that user).</li>
<li>What we are interested in in this case is to host the data that we are going to print when the image is invoked. This is why we delete all the files, to make it lighter. Images should contain only strictly what’s needed (this contains more things, anyway, since it contains all the Python modules needed to build it and Python itself.</li>
<li>We use <code>ENTRYPOINT</code> instead of <code>CMD</code>. With this, it is more similar to a normal executable, being able to receive arguments and so on.</li>
</ul>
<p>This is just a plan for building a Dockerfile, of course; you need to actually create the image to use it.</p>
<pre><code>docker build -f first.Dockerfile -t jj/nova-mlops-first .</code></pre>
<p>Then you can run it with</p>
<pre><code>docker run -t jj/nova-mlops-first</code></pre>
<p>Using <code>-t</code> will tell docker that it needs to use the console or terminal, basically that there will be something to print and it should not keep it to itself. This will, effectively, print the content of the CSV file to the screen, and you can then redirect it to a file and use it however you want.</p>
<h2 id="using-active-containers">Using “active containers”</h2>
<p>These containers are little more than data containers. You will generally want these containers to actually do something. Containers run services, applications, and can also be very useful when running tests, encapsulating everything you need to effectively run the test.</p>
<p>It can also be used for running periodic tasks such as downloading files. In this case we will need to set up a space that is shared between host and container, and where it will or file whatever is needed.</p>
<pre><code>FROM python:latest

LABEL version=0.0.1 maintainer=&quot;jjmerelo@gmail.com&quot;
RUN useradd -ms /bin/bash novamlops
USER novamlops
WORKDIR /home/novamlops
ENV PATH=~&quot;/home/novamlops/.poetry/bin:/home/novamlops/.local/bin:${PATH}&quot;

COPY --chown=novamlops pyproject.toml poetry.lock .
RUN mkdir colares_project &amp;&amp; mkdir data
COPY --chown=novamlops colares_project/ colares_project/
RUN pip install poetry \
    &amp;&amp; poetry install

VOLUME /home/mlops/data

ENTRYPOINT [&quot;poetry&quot;, &quot;run&quot;, &quot;testcsv&quot;]</code></pre>
<p>This one is very similar to the previous one</p>
<blockquote>
<p>As a matter of fact, what we should have done is to create this one, and then the other one based on this one. We might still do it.</p>
</blockquote>
<p>Main differences are</p>
<ul>
<li>We are creating a <code>data</code> directory and declaring it as a <code>VOLUME</code>. This is just an announcement, and does not have any function coupled with it.</li>
<li>We are running a <code>poetry</code> target as the entry point.</li>
<li>We are obviously not deleting anything at all.</li>
</ul>
<p>By declaring a directory as a <code>VOLUME</code> what we state is the intention to interact with the container using it.</p>
<blockquote>
<p>There are also subtle changes to the code, but that’s not really important.</p>
</blockquote>
<p>We can run it like this:</p>
<pre><code>docker run --rm -it -v `pwd`:/home/novamlops/data \
  jj/nova-mlops-second data/test.csv</code></pre>
<p>First difference is that we’re using <code>-it</code>. It’s not really needed, but it will help if we want to kill it for some reason. <code>t</code> will print to terminal (as we have seen), <code>i</code> is interactive. But the most important thing is the <code>-v</code>, which is <em>mounting</em> the <code>VOLUME</code> that we have declared in this current directory (<code>pwd</code>: present working directory, backticks run a program and use output, it’s a shell command which probably has some equivalent in PowerShell). That directory needs to be empty in the container, since it will be “mounted” to this external directory, and will have the contents of this directory.</p>
<p>But what we want is the container to produce a file with a specific name; this is added at the end of the script. Since the script is going to be running in the repository root directory, we will need to precede it with the directory name. This will, effectively, create a <code>test.csv</code> file in <code>data</code></p>
<h2 id="see-also">See also</h2>
<p>The book Docker for Data Science is a good reference (Cook 2017)</p>
<h2 id="references">References</h2>
<p>Cook, Joshua. 2017. <em>Docker for Data Science: Building Scalable and Extensible Data Infrastructure Around the Jupyter Notebook Server</em>. Apress.</p>
</body>
</html>
